{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xn_VxPiKoCqa"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative method using chatbot library \n",
        "from chatterbot import Chatbot\n",
        "from chatterbot.trainer import ListTrainer\n",
        "\n",
        "# learning based on previous dialog to generate responses "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('intents.json') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TS ['Hi', 'Hey', 'Is anyone there?', 'Hello', 'Hay', 'Bye', 'See you later', 'Goodbye', 'Thanks', 'Thank you', \"That's helpful\", 'Thanks for the help', 'Who are you?', 'What are you?', 'Who you are?', 'what is your name', 'what should I call you', 'whats your name?', 'Could you help me?', 'give me a hand please', 'Can you help?', 'What can you do for me?', 'I need a support', 'I need a help', 'support me please', 'I need to create a new account', 'how to open a new account', 'I want to create an account', 'can you create an account for me', 'how to open a new account', 'have a complaint', 'I want to raise a complaint', 'there is a complaint about a service'] \n",
            "\n",
            "['greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'goodbye', 'goodbye', 'goodbye', 'thanks', 'thanks', 'thanks', 'thanks', 'about', 'about', 'about', 'name', 'name', 'name', 'help', 'help', 'help', 'help', 'help', 'help', 'help', 'createaccount', 'createaccount', 'createaccount', 'createaccount', 'createaccount', 'complaint', 'complaint', 'complaint']\n",
            "['greeting', 'goodbye', 'thanks', 'about', 'name', 'help', 'createaccount', 'complaint']\n",
            "[['Hello', 'Hi', 'Hi there'], ['See you later', 'Have a nice day', 'Bye!'], ['Happy to help!', 'Any time!', 'My pleasure', \"You're welcome!\"], ['I.m Joana, your bot assistant', \"I'm Joana, an Artificial Intelligent bot\"], ['You can call me Joana.', \"I'm Joana!\", 'Just call me Joana'], ['Tell me how can assist you', 'Tell me your problem to assist you', 'Yes Sure, How can I support you'], ['You can just easily create a new account from our web site', 'Just go to our web site and follow the guidelines to create a new account'], ['Please provide us your complaint in order to assist you', 'Please mention your complaint, we will reach you and sorry for any inconvenience caused']]\n"
          ]
        }
      ],
      "source": [
        "training_sentences = []\n",
        "training_labels = []\n",
        "labels = []\n",
        "responses = []\n",
        "\n",
        "\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        training_sentences.append(pattern)\n",
        "        training_labels.append(intent['tag'])\n",
        "    responses.append(intent['responses'])\n",
        "    \n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])\n",
        "        \n",
        "num_classes = len(labels)\n",
        "\n",
        "print(\"TS\", training_sentences, \"\\n\")\n",
        "print(training_labels)\n",
        "print(labels)\n",
        "print(responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4, 4, 4, 4, 4, 3, 3, 3, 7, 7, 7, 7, 0, 0, 0, 6, 6, 6, 5, 5, 5, 5,\n",
              "       5, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "lbl_encoder = LabelEncoder()\n",
        "lbl_encoder.fit(training_labels) \n",
        "training_labels = lbl_encoder.transform(training_labels) # converts into data model can understand \n",
        "\n",
        "training_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_len = 20\n",
        "oov_token = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token) # adding out of vocabulary token\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential() # stack like model\n",
        "# Documentation:\n",
        "# tf.keras.layers.Embedding(input_dim, output_dim, input_length)\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len)) # search this too!\n",
        "\n",
        "# converts 3D vector to 2D vector\n",
        "model.add(GlobalAveragePooling1D())\n",
        "\n",
        "# relu acitvation function (usually default) in hidden layer to avoid vanishing gradient problem\n",
        "# non-linear activation function: advanatge is that it does not activate all the neurons at once\n",
        "model.add(Dense(16, activation='relu'))  \n",
        "model.add(Dense(16, activation='relu'))\n",
        "# softmax converts raw output into a vector of probabilities \n",
        "model.add(Dense(num_classes, activation='softmax')) # search what softmax does !\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',   \n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# print(\"here\")\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# epochs = 550\n",
        "history = model.fit(padded_sequences, np.array(training_labels), epochs=550)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to save the trained model\n",
        "# lets go through this \n",
        "model.save(\"chat_model\")\n",
        "\n",
        "# pickle used to serialize/ deserialize objects \n",
        "# pickling is a way to convert a python object into a character stream\n",
        "\n",
        "# to save the fitted tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "# to save the fitted label encoder\n",
        "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
        "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "8dd6c98dce5f873fe4a889b38658f014c1e9a969afcd1d46f7b6b9586d742aba"
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 64-bit ('tensorflow': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
